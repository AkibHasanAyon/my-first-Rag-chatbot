{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6518fb53",
   "metadata": {},
   "source": [
    "# Rag chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb586d5",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce45f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import google.api_core.exceptions\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from gemini_setup import model, system_prompt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00cb331d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: models/gemini-2.0-flash\n"
     ]
    }
   ],
   "source": [
    "print(\"Using model:\", model._model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d24715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracted_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = \"\"\n",
    "    for page in doc:\n",
    "        all_text += page.get_text(\"text\")\n",
    "    doc.close()\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38710015",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f55afb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_func(text, max_tokens=300, overlap=50):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    chunks, chunk = [], []\n",
    "    total_words = 0\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if total_words + len(words) > max_tokens:\n",
    "            chunks.append(' '.join(chunk))\n",
    "            if overlap > 0:\n",
    "                chunk = ' '.join(chunk[-overlap:]).split()\n",
    "                total_words = len(chunk)\n",
    "            else:\n",
    "                chunk, total_words = [], 0\n",
    "        chunk.extend(words)\n",
    "        total_words += len(words)\n",
    "    if chunk:\n",
    "        chunks.append(' '.join(chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81d9045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"A Comprehensive Analysis of Liver Disease Detection Using Advanced Machine Learning Algorithms.pdf\"\n",
    "extracted_text = extracted_pdf(pdf_path)\n",
    "chunk_text = chunk_text_func(extracted_text, max_tokens=300, overlap=50)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "response = embedding_model.encode(chunk_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc5d3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def semantic_search(query, chunk_text, embeddings, k=2):\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    scores = [(i, cosine_similarity(query_embedding, emb)) for i, emb in enumerate(embeddings)]\n",
    "    top_indices = sorted(scores, key=lambda x: x[1], reverse=True)[:k]\n",
    "    return [chunk_text[i] for i, _ in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8ff2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_gemini(system_prompt, top_chunks, query, max_chunk_chars=1200, max_retries=3):\n",
    "    truncated_chunks = [c[:max_chunk_chars] for c in top_chunks]\n",
    "    user_prompt = \"\\n\".join([f\"Context {i+1}:\\n{c}\\n=====================\" for i, c in enumerate(truncated_chunks)])\n",
    "    user_prompt += f\"\\n\\nQuestion: {query}\"\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    for attempt in range(1, max_retries+1):\n",
    "        try:\n",
    "            response = model.generate_content(full_prompt)\n",
    "            return response.text.strip()\n",
    "        except google.api_core.exceptions.ResourceExhausted:\n",
    "            wait = 10 * attempt\n",
    "            print(f\"‚ö†Ô∏è ResourceExhausted: Waiting {wait}s (Attempt {attempt}/{max_retries})\")\n",
    "            time.sleep(wait)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error: {e}\")\n",
    "            return f\"ERROR: {e}\"\n",
    "    return \"‚ùå ERROR: Failed after multiple retries\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e7b87ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"val.json\", \"r\") as f:\n",
    "    full_data = json.load(f)\n",
    "    data = full_data[:10]  # only 10 questions\n",
    "\n",
    "all_responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d37b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini answers:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:09<00:13,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved 5 partial responses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini answers:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:19<00:02,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved 10 partial responses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini answers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:21<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved all 10 answers to gemini_responses_10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(tqdm(data, desc=\"Generating Gemini answers\")):\n",
    "    query = item['question']\n",
    "    top_chunks = semantic_search(query, chunk_text, response, k=2)\n",
    "    gemini_reply = generate_response_gemini(system_prompt, top_chunks, query)\n",
    "\n",
    "    all_responses.append({\n",
    "        \"question\": query,\n",
    "        \"answer\": gemini_reply\n",
    "    })\n",
    "\n",
    "    if len(all_responses) % 5 == 0:\n",
    "        with open(\"gemini_partial.json\", \"w\") as f:\n",
    "            json.dump(all_responses, f, indent=2)\n",
    "        print(f\"üíæ Saved {len(all_responses)} partial responses\")\n",
    "\n",
    "    time.sleep(1.2)  # to avoid rate limit\n",
    "\n",
    "with open(\"gemini_responses_10.json\", \"w\") as f:\n",
    "    json.dump(all_responses, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Saved all 10 answers to gemini_responses_10.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eac2988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_system_prompt = (\n",
    "    \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. \"\n",
    "    \"If the AI assistant's response is very close to the true response, assign a score of 1. \"\n",
    "    \"If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. \"\n",
    "    \"If the response is partially aligned with the true response, assign a score of 0.5.\\n\\n\"\n",
    "    \"Only return the number (0, 0.5, or 1).\"\n",
    ")\n",
    "\n",
    "def evaluate_response(query, gemini_reply, true_answer, max_retries=3):\n",
    "    eval_prompt = (\n",
    "        f\"User Query: {query}\\n\"\n",
    "        f\"AI Response:\\n{gemini_reply}\\n\"\n",
    "        f\"True Response:\\n{true_answer}\\n\\n\"\n",
    "        f\"{evaluate_system_prompt}\"\n",
    "    )\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = model.generate_content(eval_prompt)\n",
    "            return result.text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Eval retry ({attempt+1}/3): {e}\")\n",
    "            time.sleep(5)\n",
    "    return \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54775562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Eval retry (1/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 48\n",
      "}\n",
      "]\n",
      "‚úÖ Evaluations saved to gemini_evaluations_10.json\n"
     ]
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    question = data[i][\"question\"]\n",
    "    ideal_answer = data[i][\"ideal_answer\"]\n",
    "    generated_answer = all_responses[i][\"answer\"]\n",
    "    \n",
    "    score = evaluate_response(question, generated_answer, ideal_answer)\n",
    "    \n",
    "    evaluations.append({\n",
    "        \"question\": question,\n",
    "        \"ideal_answer\": ideal_answer,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"score\": score\n",
    "    })\n",
    "\n",
    "with open(\"gemini_evaluations_10.json\", \"w\") as f:\n",
    "    json.dump(evaluations, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Evaluations saved to gemini_evaluations_10.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2ca81",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'evaluation_results.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load evaluation results from file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mevaluation_results.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      5\u001b[39m     eval_data = json.load(f)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Parse numeric scores only (ignore invalid ones)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\my files\\vs project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'evaluation_results.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load evaluation results from file\n",
    "with open(\"gemini_evaluations_10.json\", \"r\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Parse numeric scores only (ignore invalid ones)\n",
    "scores = []\n",
    "for item in eval_data:\n",
    "    try:\n",
    "        score = float(item[\"score\"])\n",
    "        if score in [0, 0.5, 1.0]:\n",
    "            scores.append(score)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Compute average score\n",
    "if scores:\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"‚úÖ Average Evaluation Score: {average_score:.2f} (from {len(scores)} responses)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid scores found in evaluation_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b914d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
